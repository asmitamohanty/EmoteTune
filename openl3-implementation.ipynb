{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10165440,"sourceType":"datasetVersion","datasetId":6225532},{"sourceId":10185717,"sourceType":"datasetVersion","datasetId":6158297}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openl3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:03:10.103544Z","iopub.execute_input":"2024-12-12T05:03:10.104026Z","iopub.status.idle":"2024-12-12T05:04:07.453772Z","shell.execute_reply.started":"2024-12-12T05:03:10.103993Z","shell.execute_reply":"2024-12-12T05:04:07.452683Z"}},"outputs":[{"name":"stdout","text":"Collecting openl3\n  Downloading openl3-0.4.2.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorflow>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from openl3) (2.16.1)\nRequirement already satisfied: numpy>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from openl3) (1.26.4)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from openl3) (1.14.1)\nCollecting kapre>=0.3.5 (from openl3)\n  Downloading kapre-0.3.7.tar.gz (26 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: soundfile>=0.9.0.post1 in /opt/conda/lib/python3.10/site-packages (from openl3) (0.12.1)\nCollecting resampy<0.3.0,>=0.2.1 (from openl3)\n  Downloading resampy-0.2.2.tar.gz (323 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.4/323.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: h5py>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from openl3) (3.11.0)\nCollecting moviepy>=1.0.0 (from openl3)\n  Downloading moviepy-2.1.1-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: scikit-image>=0.14.3 in /opt/conda/lib/python3.10/site-packages (from openl3) (0.23.2)\nRequirement already satisfied: librosa>=0.7.2 in /opt/conda/lib/python3.10/site-packages (from openl3) (0.10.2.post1)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (0.60.0)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (0.5.0.post1)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (4.12.2)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.7.2->openl3) (1.0.8)\nRequirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy>=1.0.0->openl3) (2.34.1)\nCollecting imageio_ffmpeg>=0.2.0 (from moviepy>=1.0.0->openl3)\n  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\nCollecting proglog<=1.0.0 (from moviepy>=1.0.0->openl3)\n  Downloading proglog-0.1.10-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: python-dotenv>=0.10 in /opt/conda/lib/python3.10/site-packages (from moviepy>=1.0.0->openl3) (1.0.1)\nRequirement already satisfied: pillow<11.0,>=9.2.0 in /opt/conda/lib/python3.10/site-packages (from moviepy>=1.0.0->openl3) (10.3.0)\nRequirement already satisfied: six>=1.3 in /opt/conda/lib/python3.10/site-packages (from resampy<0.3.0,>=0.2.1->openl3) (1.16.0)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.3->openl3) (3.3)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.3->openl3) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.3->openl3) (21.3)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.9.0.post1->openl3) (1.16.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (0.3.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (2.32.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (70.0.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (2.4.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (1.62.2)\nRequirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (2.16.2)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (3.3.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.0.0->openl3) (0.37.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->openl3) (0.43.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.9.0.post1->openl3) (2.22)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow>=2.0.0->openl3) (13.7.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow>=2.0.0->openl3) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow>=2.0.0->openl3) (0.11.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa>=0.7.2->openl3) (0.43.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image>=0.14.3->openl3) (3.1.2)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa>=0.7.2->openl3) (3.11.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from proglog<=1.0.0->moviepy>=1.0.0->openl3) (4.66.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0->openl3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0->openl3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0->openl3) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0->openl3) (2024.6.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa>=0.7.2->openl3) (3.5.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.0.0->openl3) (3.6)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.0.0->openl3) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.0.0->openl3) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow>=2.0.0->openl3) (2.1.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow>=2.0.0->openl3) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow>=2.0.0->openl3) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow>=2.0.0->openl3) (0.1.2)\nDownloading moviepy-2.1.1-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\nBuilding wheels for collected packages: openl3, kapre, resampy\n  Building wheel for openl3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for openl3: filename=openl3-0.4.2-py2.py3-none-any.whl size=249327032 sha256=a78070a890e1b4505ad6b16ad12fd7f79a85bb87088b5de1f3feb4470025bd74\n  Stored in directory: /root/.cache/pip/wheels/d0/4d/0a/e57b1dc8ead91b3c5709d9de4f02d1cdd3a91f609a8f1c1062\n  Building wheel for kapre (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for kapre: filename=kapre-0.3.7-py3-none-any.whl size=29603 sha256=587c2cb920124a054afa1ef2757e6de261a972d3d422bf480d596e07a19437a9\n  Stored in directory: /root/.cache/pip/wheels/3e/54/f9/37e9f36590a5431dc207f7dc0eb2e607ff8958d5728e45ef58\n  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320708 sha256=407e86eed678da6969b5acbcbe9a1b97cbdd41d647487b81656274d452e88550\n  Stored in directory: /root/.cache/pip/wheels/e5/a0/79/29e61754e5b3941ad4c7d01bf5bea99768e64e4bdd3180f32b\nSuccessfully built openl3 kapre resampy\nInstalling collected packages: proglog, imageio_ffmpeg, resampy, moviepy, kapre, openl3\nSuccessfully installed imageio_ffmpeg-0.5.1 kapre-0.3.7 moviepy-2.1.1 openl3-0.4.2 proglog-0.1.10 resampy-0.2.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip show openl3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:07.455575Z","iopub.execute_input":"2024-12-12T05:04:07.455937Z","iopub.status.idle":"2024-12-12T05:04:15.418179Z","shell.execute_reply.started":"2024-12-12T05:04:07.455891Z","shell.execute_reply":"2024-12-12T05:04:15.416787Z"}},"outputs":[{"name":"stdout","text":"Name: openl3\nVersion: 0.4.2\nSummary: Deep audio and image embeddings, based on Look, Listen, and Learn approach\nHome-page: https://github.com/marl/openl3\nAuthor: Aurora Cramer, Ho-Hsiang Wu, Bea Steers, and Justin Salamon\nAuthor-email: jtc440@nyu.edu\nLicense: MIT\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: h5py, kapre, librosa, moviepy, numpy, resampy, scikit-image, scipy, soundfile, tensorflow\nRequired-by: \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install torchopenl3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:15.419792Z","iopub.execute_input":"2024-12-12T05:04:15.420212Z","iopub.status.idle":"2024-12-12T05:04:25.855391Z","shell.execute_reply.started":"2024-12-12T05:04:15.420169Z","shell.execute_reply":"2024-12-12T05:04:25.854093Z"}},"outputs":[{"name":"stdout","text":"Collecting torchopenl3\n  Downloading torchopenl3-1.0.1-py2.py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: numpy>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from torchopenl3) (1.26.4)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from torchopenl3) (1.14.1)\nRequirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (from torchopenl3) (0.12.1)\nRequirement already satisfied: resampy<0.3.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from torchopenl3) (0.2.2)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from torchopenl3) (2.4.0)\nCollecting nnAudio>=0.2.4 (from torchopenl3)\n  Downloading nnAudio-0.3.3-py3-none-any.whl.metadata (771 bytes)\nCollecting julius>=0.2.5 (from torchopenl3)\n  Downloading julius-0.2.7.tar.gz (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (from torchopenl3) (0.10.2.post1)\nRequirement already satisfied: numba>=0.32 in /opt/conda/lib/python3.10/site-packages (from resampy<0.3.0,>=0.2.1->torchopenl3) (0.60.0)\nRequirement already satisfied: six>=1.3 in /opt/conda/lib/python3.10/site-packages (from resampy<0.3.0,>=0.2.1->torchopenl3) (1.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchopenl3) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchopenl3) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchopenl3) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchopenl3) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchopenl3) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchopenl3) (2024.6.0)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa->torchopenl3) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa->torchopenl3) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa->torchopenl3) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa->torchopenl3) (5.1.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa->torchopenl3) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa->torchopenl3) (0.5.0.post1)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa->torchopenl3) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->torchopenl3) (1.0.8)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile->torchopenl3) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile->torchopenl3) (2.22)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from lazy-loader>=0.1->librosa->torchopenl3) (21.3)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.32->resampy<0.3.0,>=0.2.1->torchopenl3) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa->torchopenl3) (3.11.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa->torchopenl3) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa->torchopenl3) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->torchopenl3) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->torchopenl3) (1.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->lazy-loader>=0.1->librosa->torchopenl3) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->torchopenl3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->torchopenl3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->torchopenl3) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa->torchopenl3) (2024.6.2)\nDownloading torchopenl3-1.0.1-py2.py3-none-any.whl (18 kB)\nDownloading nnAudio-0.3.3-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: julius\n  Building wheel for julius (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=681b64b5fb649184ab67543a226871b5d70aef7411ce57772bdeaba75eec02bf\n  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\nSuccessfully built julius\nInstalling collected packages: nnAudio, julius, torchopenl3\nSuccessfully installed julius-0.2.7 nnAudio-0.3.3 torchopenl3-1.0.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip show torchopenl3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:25.857747Z","iopub.execute_input":"2024-12-12T05:04:25.858109Z","iopub.status.idle":"2024-12-12T05:04:34.154838Z","shell.execute_reply.started":"2024-12-12T05:04:25.858075Z","shell.execute_reply":"2024-12-12T05:04:34.153774Z"}},"outputs":[{"name":"stdout","text":"Name: torchopenl3\nVersion: 1.0.1\nSummary: Deep audio and image embeddings, based on Look, Listen, and Learn approach Pytorch\nHome-page: https://github.com/torchopenl3/torchopenl3/\nAuthor: Humair Raj Khan and Gyanendra Das\nAuthor-email: gyanendralucky9337@gmail.com\nLicense: MIT\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: julius, librosa, nnAudio, numpy, resampy, scipy, soundfile, torch\nRequired-by: \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport zipfile\nimport openl3\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:34.156392Z","iopub.execute_input":"2024-12-12T05:04:34.156862Z","iopub.status.idle":"2024-12-12T05:04:49.749842Z","shell.execute_reply.started":"2024-12-12T05:04:34.156793Z","shell.execute_reply":"2024-12-12T05:04:49.748770Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torchopenl3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:49.751009Z","iopub.execute_input":"2024-12-12T05:04:49.751499Z","iopub.status.idle":"2024-12-12T05:04:49.765559Z","shell.execute_reply.started":"2024-12-12T05:04:49.751470Z","shell.execute_reply":"2024-12-12T05:04:49.764919Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(\"GPU is available and being used by PyTorch!\")\nelse:\n    print(\"GPU is NOT available. Please check your Kaggle settings.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:49.767020Z","iopub.execute_input":"2024-12-12T05:04:49.767285Z","iopub.status.idle":"2024-12-12T05:04:49.844520Z","shell.execute_reply.started":"2024-12-12T05:04:49.767250Z","shell.execute_reply":"2024-12-12T05:04:49.843495Z"}},"outputs":[{"name":"stdout","text":"GPU is available and being used by PyTorch!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"input_folder = \"/kaggle/input/raw-00-01-02-filtered-files\"  \nembedding_dir = \"/kaggle/working/embeddings\"\ntsv_file = \"/kaggle/input/moodtheme-1/final_mood_labels.tsv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:49.845747Z","iopub.execute_input":"2024-12-12T05:04:49.846052Z","iopub.status.idle":"2024-12-12T05:04:49.855301Z","shell.execute_reply.started":"2024-12-12T05:04:49.846024Z","shell.execute_reply":"2024-12-12T05:04:49.854521Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\ndf = pd.read_csv(tsv_file, sep='\\t', header=None, names=['file', 'label'])\nif df.iloc[0, 0] == \"PATH\":\n    df = df.drop(index=0).reset_index(drop=True)\n\ndf['name'] = df['file'].apply(lambda x: x.split('/')[-1])  \n\n\nname_to_label = dict(zip(df['name'], df['label']))\n\n\n\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:49.856355Z","iopub.execute_input":"2024-12-12T05:04:49.856603Z","iopub.status.idle":"2024-12-12T05:04:49.907708Z","shell.execute_reply.started":"2024-12-12T05:04:49.856579Z","shell.execute_reply":"2024-12-12T05:04:49.906872Z"}},"outputs":[{"name":"stdout","text":"                 file      label         name\n0          48/948.mp3  energetic      948.mp3\n1          50/950.mp3  energetic      950.mp3\n2          51/951.mp3  energetic      951.mp3\n3         66/2166.mp3  energetic     2166.mp3\n4         47/6247.mp3       calm     6247.mp3\n...               ...        ...          ...\n11587  56/1422056.mp3      happy  1422056.mp3\n11588  57/1422057.mp3      happy  1422057.mp3\n11589  58/1422058.mp3  energetic  1422058.mp3\n11590  59/1422059.mp3      happy  1422059.mp3\n11591  60/1422060.mp3      happy  1422060.mp3\n\n[11592 rows x 3 columns]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:49.910329Z","iopub.execute_input":"2024-12-12T05:04:49.910969Z","iopub.status.idle":"2024-12-12T05:04:49.924426Z","shell.execute_reply.started":"2024-12-12T05:04:49.910938Z","shell.execute_reply":"2024-12-12T05:04:49.923577Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"label\nhappy        6291\nenergetic    3046\ncalm         1890\ntense         365\nName: count, dtype: int64"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:49.925463Z","iopub.execute_input":"2024-12-12T05:04:49.925704Z","iopub.status.idle":"2024-12-12T05:04:50.972458Z","shell.execute_reply.started":"2024-12-12T05:04:49.925679Z","shell.execute_reply":"2024-12-12T05:04:50.971543Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\nfinal_data = []\n\n\nfor root, dirs, files in os.walk(input_folder):\n    #print(root)\n    for file in files:\n        if file.endswith('.mp3'):\n            name = file.replace('low.','') #os.path.splitext(file)[0]  \n            #print(\"name:\", name)\n            if name in name_to_label:\n                full_path = os.path.join(root, file)\n                label = name_to_label[name]\n                #print(full_path, label)\n                final_data.append([full_path, label]) \n\n\nfinal_df = pd.DataFrame(final_data, columns=['audio_file_path', 'label'])\n\n\noutput_file = '/kaggle/working/audio_files_with_labels.tsv'\n\n\nprint(f\"Final file saved to: {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:04:50.974038Z","iopub.execute_input":"2024-12-12T05:04:50.974467Z","iopub.status.idle":"2024-12-12T05:05:10.866098Z","shell.execute_reply.started":"2024-12-12T05:04:50.974428Z","shell.execute_reply":"2024-12-12T05:05:10.865125Z"}},"outputs":[{"name":"stdout","text":"Final file saved to: /kaggle/working/audio_files_with_labels.tsv\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"final_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:10.867192Z","iopub.execute_input":"2024-12-12T05:05:10.867482Z","iopub.status.idle":"2024-12-12T05:05:10.880077Z","shell.execute_reply.started":"2024-12-12T05:05:10.867455Z","shell.execute_reply":"2024-12-12T05:05:10.879347Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                        audio_file_path      label\n0     /kaggle/input/raw-00-01-02-filtered-files/filt...  energetic\n1     /kaggle/input/raw-00-01-02-filtered-files/filt...      happy\n2     /kaggle/input/raw-00-01-02-filtered-files/filt...      tense\n3     /kaggle/input/raw-00-01-02-filtered-files/filt...      happy\n4     /kaggle/input/raw-00-01-02-filtered-files/filt...       calm\n...                                                 ...        ...\n7177  /kaggle/input/raw-00-01-02-filtered-files/filt...      happy\n7178  /kaggle/input/raw-00-01-02-filtered-files/filt...  energetic\n7179  /kaggle/input/raw-00-01-02-filtered-files/filt...       calm\n7180  /kaggle/input/raw-00-01-02-filtered-files/filt...      happy\n7181  /kaggle/input/raw-00-01-02-filtered-files/filt...      happy\n\n[7182 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio_file_path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>energetic</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>tense</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>calm</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7177</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>7178</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>energetic</td>\n    </tr>\n    <tr>\n      <th>7179</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>calm</td>\n    </tr>\n    <tr>\n      <th>7180</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>7181</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>happy</td>\n    </tr>\n  </tbody>\n</table>\n<p>7182 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"final_df['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:10.880983Z","iopub.execute_input":"2024-12-12T05:05:10.881227Z","iopub.status.idle":"2024-12-12T05:05:10.900181Z","shell.execute_reply.started":"2024-12-12T05:05:10.881202Z","shell.execute_reply":"2024-12-12T05:05:10.899395Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"label\nhappy        4012\nenergetic    1886\ncalm         1098\ntense         186\nName: count, dtype: int64"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import librosa\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:10.901458Z","iopub.execute_input":"2024-12-12T05:05:10.901879Z","iopub.status.idle":"2024-12-12T05:05:10.910995Z","shell.execute_reply.started":"2024-12-12T05:05:10.901835Z","shell.execute_reply":"2024-12-12T05:05:10.910175Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\ndf = final_df.copy()  \n\n\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['label'])\n\ndf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:10.911945Z","iopub.execute_input":"2024-12-12T05:05:10.912189Z","iopub.status.idle":"2024-12-12T05:05:10.931586Z","shell.execute_reply.started":"2024-12-12T05:05:10.912164Z","shell.execute_reply":"2024-12-12T05:05:10.930695Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                        audio_file_path  label\n0     /kaggle/input/raw-00-01-02-filtered-files/filt...      1\n1     /kaggle/input/raw-00-01-02-filtered-files/filt...      2\n2     /kaggle/input/raw-00-01-02-filtered-files/filt...      3\n3     /kaggle/input/raw-00-01-02-filtered-files/filt...      2\n4     /kaggle/input/raw-00-01-02-filtered-files/filt...      0\n...                                                 ...    ...\n7177  /kaggle/input/raw-00-01-02-filtered-files/filt...      2\n7178  /kaggle/input/raw-00-01-02-filtered-files/filt...      1\n7179  /kaggle/input/raw-00-01-02-filtered-files/filt...      0\n7180  /kaggle/input/raw-00-01-02-filtered-files/filt...      2\n7181  /kaggle/input/raw-00-01-02-filtered-files/filt...      2\n\n[7182 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio_file_path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7177</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7178</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7179</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7180</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7181</th>\n      <td>/kaggle/input/raw-00-01-02-filtered-files/filt...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>7182 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:10.932564Z","iopub.execute_input":"2024-12-12T05:05:10.932791Z","iopub.status.idle":"2024-12-12T05:05:10.943659Z","shell.execute_reply.started":"2024-12-12T05:05:10.932767Z","shell.execute_reply":"2024-12-12T05:05:10.942950Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"label\n2    4012\n1    1886\n0    1098\n3     186\nName: count, dtype: int64"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"audio0, label0 = df['audio_file_path'].iloc[0], df['label'].iloc[0]\nprint(audio0, label0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:10.944504Z","iopub.execute_input":"2024-12-12T05:05:10.944714Z","iopub.status.idle":"2024-12-12T05:05:10.956772Z","shell.execute_reply.started":"2024-12-12T05:05:10.944692Z","shell.execute_reply":"2024-12-12T05:05:10.956011Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/raw-00-01-02-filtered-files/filtered_files_16-20/749618.low.mp3 1\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:10.957609Z","iopub.execute_input":"2024-12-12T05:05:10.957862Z","iopub.status.idle":"2024-12-12T05:05:10.967353Z","shell.execute_reply.started":"2024-12-12T05:05:10.957833Z","shell.execute_reply":"2024-12-12T05:05:10.966538Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"wave_audio0, sr0 = librosa.load(audio0, sr=16000)\nprint(wave_audio0, sr0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:10.968273Z","iopub.execute_input":"2024-12-12T05:05:10.968510Z","iopub.status.idle":"2024-12-12T05:05:22.087767Z","shell.execute_reply.started":"2024-12-12T05:05:10.968486Z","shell.execute_reply":"2024-12-12T05:05:22.086853Z"}},"outputs":[{"name":"stdout","text":"[ 0.00010839  0.00050483  0.00070658 ... -0.00243625 -0.00268697\n  0.        ] 16000\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(wave_audio0.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:22.089123Z","iopub.execute_input":"2024-12-12T05:05:22.089474Z","iopub.status.idle":"2024-12-12T05:05:22.093617Z","shell.execute_reply.started":"2024-12-12T05:05:22.089444Z","shell.execute_reply":"2024-12-12T05:05:22.092891Z"}},"outputs":[{"name":"stdout","text":"(5331628,)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"wave_audio = torch.tensor(wave_audio0, dtype=torch.float32).unsqueeze(0).to(device)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:22.094778Z","iopub.execute_input":"2024-12-12T05:05:22.095151Z","iopub.status.idle":"2024-12-12T05:05:22.333890Z","shell.execute_reply.started":"2024-12-12T05:05:22.095109Z","shell.execute_reply":"2024-12-12T05:05:22.333052Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"input_repr = 'mel256'  \ncontent_type = 'music'  \nembedding_size = 512  \n\n# Load the model\nmodel = torchopenl3.models.load_audio_embedding_model(\n    input_repr=input_repr,\n    content_type=content_type,\n    embedding_size=embedding_size\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:22.335081Z","iopub.execute_input":"2024-12-12T05:05:22.335507Z","iopub.status.idle":"2024-12-12T05:05:24.420829Z","shell.execute_reply.started":"2024-12-12T05:05:22.335461Z","shell.execute_reply":"2024-12-12T05:05:24.419774Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchopenl3/models.py:200: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n  mel_basis = librosa.filters.mel(\nDownloading: \"https://github.com/torchopenl3/torchopenl3-models/raw/master/torchopenl3_mel256_music_512.pth.tar\" to /root/.cache/torch/hub/checkpoints/torchopenl3_mel256_music_512.pth.tar\n100%|██████████| 34.9M/34.9M [00:00<00:00, 270MB/s]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"model=model.to(device)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:24.422035Z","iopub.execute_input":"2024-12-12T05:05:24.422400Z","iopub.status.idle":"2024-12-12T05:05:24.445211Z","shell.execute_reply.started":"2024-12-12T05:05:24.422369Z","shell.execute_reply":"2024-12-12T05:05:24.444058Z"}},"outputs":[{"name":"stdout","text":"PytorchOpenl3(\n  (speclayer): CustomMelSTFT()\n  (batch_normalization_1): BatchNorm2d(1, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n  (conv2d_1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n  (batch_normalization_2): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n  (conv2d_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n  (batch_normalization_3): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n  (conv2d_3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n  (batch_normalization_4): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n  (conv2d_4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n  (batch_normalization_5): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n  (conv2d_5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n  (batch_normalization_6): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n  (conv2d_6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n  (batch_normalization_7): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n  (conv2d_7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n  (batch_normalization_8): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n  (audio_embedding_layer): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import random\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:24.446203Z","iopub.execute_input":"2024-12-12T05:05:24.446484Z","iopub.status.idle":"2024-12-12T05:05:24.450557Z","shell.execute_reply.started":"2024-12-12T05:05:24.446457Z","shell.execute_reply":"2024-12-12T05:05:24.449581Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"file_paths = df['audio_file_path'].tolist()\nlabels = df['label'].tolist()\n\nshuffled_paths, shuffled_labels = zip(*random.sample(list(zip(file_paths, labels)), len(file_paths)))\n\n\ntrain_paths, temp_paths, train_labels, temp_labels = train_test_split(shuffled_paths, shuffled_labels, test_size=0.2, random_state=42)\nval_paths, test_paths, val_labels, test_labels = train_test_split(temp_paths, temp_labels, test_size=0.5, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:24.451357Z","iopub.execute_input":"2024-12-12T05:05:24.451657Z","iopub.status.idle":"2024-12-12T05:05:24.471783Z","shell.execute_reply.started":"2024-12-12T05:05:24.451615Z","shell.execute_reply":"2024-12-12T05:05:24.471083Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"print(len(train_paths), len(val_paths), len(test_paths))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:24.473332Z","iopub.execute_input":"2024-12-12T05:05:24.473669Z","iopub.status.idle":"2024-12-12T05:05:24.484135Z","shell.execute_reply.started":"2024-12-12T05:05:24.473643Z","shell.execute_reply":"2024-12-12T05:05:24.483281Z"}},"outputs":[{"name":"stdout","text":"5745 718 719\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"embeddings, _ = torchopenl3.get_audio_embedding(\n    torch.tensor(wave_audio), \n    model=model, \n    sr=22050  \n)\n\n\nprint(embeddings.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:05:54.774603Z","iopub.execute_input":"2024-12-12T05:05:54.775330Z","iopub.status.idle":"2024-12-12T05:06:11.949378Z","shell.execute_reply.started":"2024-12-12T05:05:54.775294Z","shell.execute_reply":"2024-12-12T05:06:11.948472Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/169508312.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(wave_audio),\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 2414, 512])\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from tqdm import tqdm\nclass AudioDataset(torch.utils.data.Dataset):\n    def __init__(self, file_paths, labels, model, device, sr=16000):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.model = model\n        self.device = device  \n        self.sr = sr\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        \n        audio_path = self.file_paths[idx]\n        audio, _ = librosa.load(audio_path, sr=self.sr)\n\n        \n        audio_tensor = torch.tensor(audio[:self.sr * 10], dtype=torch.float32).unsqueeze(0).to(self.device)\n\n        \n        embeddings, _ = torchopenl3.get_audio_embedding(audio_tensor, model=self.model, sr=self.sr)\n\n        \n        label = torch.tensor(self.labels[idx], dtype=torch.long).to(self.device)\n\n        return embeddings, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:09:31.674993Z","iopub.execute_input":"2024-12-12T05:09:31.675618Z","iopub.status.idle":"2024-12-12T05:09:31.682750Z","shell.execute_reply.started":"2024-12-12T05:09:31.675584Z","shell.execute_reply":"2024-12-12T05:09:31.681862Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def save_embeddings_in_batches(file_paths, labels, model, device, batch_size=32, sr=16000, save_interval=1000, output_dir=\"\"):\n    \n    dataset = AudioDataset(file_paths, labels, model, device, sr)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    all_embeddings = []\n    all_labels = []\n\n    for i, (audio_embeddings, batch_labels) in enumerate(tqdm(dataloader)):\n        all_embeddings.append(audio_embeddings)\n        all_labels.append(batch_labels)\n\n        \n        if (i + 1) % save_interval == 0:\n            embeddings_batch = torch.cat(all_embeddings, dim=0).cpu()  \n            labels_batch = torch.cat(all_labels, dim=0).cpu()\n            torch.save(embeddings_batch, f\"{output_dir}/audio_embeddings_batch_{i + 1}.pt\")\n            torch.save(labels_batch, f\"{output_dir}/labels_batch_{i + 1}.pt\")\n            all_embeddings = []  \n            all_labels = []\n\n    \n    final_embeddings = torch.cat(all_embeddings, dim=0).cpu()\n    final_labels = torch.cat(all_labels, dim=0).cpu()\n    torch.save(final_embeddings, f\"{output_dir}/audio_embeddings_final.pt\")\n    torch.save(final_labels, f\"{output_dir}/labels_final.pt\")\n\n    return final_embeddings, final_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:09:33.824375Z","iopub.execute_input":"2024-12-12T05:09:33.824772Z","iopub.status.idle":"2024-12-12T05:09:33.832109Z","shell.execute_reply.started":"2024-12-12T05:09:33.824740Z","shell.execute_reply":"2024-12-12T05:09:33.831102Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"\noutput_dir = '/kaggle/working/'\nembeddings, final_labels = save_embeddings_in_batches(\n    train_paths,\n    train_labels,\n    model,\n    device=device,  \n    batch_size=32,\n    sr=16000,\n    save_interval=1000,\n    output_dir=output_dir\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:37:14.531383Z","iopub.execute_input":"2024-12-11T16:37:14.531729Z","iopub.status.idle":"2024-12-11T18:07:16.295507Z","shell.execute_reply.started":"2024-12-11T16:37:14.531702Z","shell.execute_reply":"2024-12-11T18:07:16.294545Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 180/180 [1:29:59<00:00, 30.00s/it]\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"output_dir = '/kaggle/working/test_embedding'\nos.makedirs(output_dir, exist_ok=True)\ntest_embeddings, test_labels = save_embeddings_in_batches(\n    test_paths,\n    test_labels,\n    model,\n    device=device,\n    batch_size=32,\n    sr=16000,\n    save_interval=1000,\n    output_dir=output_dir\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:45:42.410975Z","iopub.execute_input":"2024-12-11T18:45:42.411368Z","iopub.status.idle":"2024-12-11T18:57:26.984883Z","shell.execute_reply.started":"2024-12-11T18:45:42.411330Z","shell.execute_reply":"2024-12-11T18:57:26.984149Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 23/23 [11:44<00:00, 30.62s/it]\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"output_dir = '/kaggle/working/validation_embedding'\nos.makedirs(output_dir, exist_ok=True)\nval_embeddings, val_labels = save_embeddings_in_batches(\n    val_paths,\n    val_labels,\n    model,\n    device=device,\n    batch_size=32,\n    sr=16000,\n    save_interval=1000,\n    output_dir=output_dir\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:09:43.867883Z","iopub.execute_input":"2024-12-12T05:09:43.868238Z","iopub.status.idle":"2024-12-12T05:21:00.640776Z","shell.execute_reply.started":"2024-12-12T05:09:43.868208Z","shell.execute_reply":"2024-12-12T05:21:00.640065Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 23/23 [11:16<00:00, 29.41s/it]\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"Code to extract PANN embeddings (used to explore PANN - not utilized thouroughly)","metadata":{}},{"cell_type":"code","source":"from torch.hub import load\n\n\nclass AudioDataset(Dataset):\n    def __init__(self, file_paths, labels=None, sr=32000):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.sr = sr\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        audio_path = self.file_paths[idx]\n        \n        audio, _ = librosa.load(audio_path, sr=self.sr)\n        \n        max_length = self.sr * 10  \n        if len(audio) > max_length:\n            audio = audio[:max_length]\n        else:\n            audio = np.pad(audio, (0, max_length - len(audio)), 'constant')\n        \n        audio_tensor = torch.tensor(audio, dtype=torch.float32)\n        if self.labels is not None:\n            return audio_tensor, self.labels[idx]\n        return audio_tensor\n\n\ndef process_and_save_embeddings(file_paths, labels, model, batch_size, output_path, sr=32000):\n    \n    dataset = AudioDataset(file_paths, labels, sr)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    embeddings_list = []\n    labels_list = [] if labels is not None else None\n\n    os.makedirs(output_path, exist_ok=True)\n    saved_batches = 0\n\n    \n    for batch in dataloader:\n        audio_tensors = batch[0] if labels is not None else batch\n        batch_labels = batch[1] if labels is not None else None\n        \n        \n        with torch.no_grad():\n            audio_embeddings = model(audio_tensors)  \n            embeddings_list.append(audio_embeddings)\n            if labels_list is not None:\n                labels_list.append(batch_labels)\n\n        \n        saved_batches += 1\n        batch_embeddings_path = os.path.join(output_path, f\"embeddings_batch_{saved_batches}.pt\")\n        torch.save(audio_embeddings, batch_embeddings_path)\n        print(f\"Saved batch {saved_batches} embeddings to {batch_embeddings_path}\")\n        \n    \n    embeddings_tensor = torch.cat(embeddings_list, dim=0)\n    torch.save(embeddings_tensor, os.path.join(output_path, \"final_embeddings.pt\"))\n    print(f\"Saved final embeddings to {os.path.join(output_path, 'final_embeddings.pt')}\")\n    \n    if labels_list is not None:\n        labels_tensor = torch.cat(labels_list, dim=0)\n        torch.save(labels_tensor, os.path.join(output_path, \"final_labels.pt\"))\n        print(f\"Saved labels to {os.path.join(output_path, 'final_labels.pt')}\")\n\n\ndef load_panns_model():\n    \n    model = load('qiuqiangkong/audioset_tagging_cnn', 'mobilenetv1_1k_mel', source='github')\n    model.eval()  \n    return model\n\n\nif __name__ == \"__main__\":\n    \n    train_file_paths = train_paths\n   \n    \n    train_labels = train_labels  \n    \n    \n    model = load_panns_model()\n    \n   \n    process_and_save_embeddings(train_paths, train_labels, model, batch_size=8, output_path=\"embeddings/train\", sr=32000)\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}